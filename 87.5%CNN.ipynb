{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ecf90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2 as cv\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import pickle\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from glob import glob \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d919234",
   "metadata": {},
   "source": [
    "Some learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d36ddddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS = 3\n",
    "#TRAIN_SIZE=80000\n",
    "TRAIN_SIZE = 8000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5c24c",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97caf6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    130908\n",
       "1     89117\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I used cropped images\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "train_labels['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3cb32a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.594969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0  0.594969\n",
       "1  0.405031"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_labels.label.value_counts() / len(train_labels)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775bf57",
   "metadata": {},
   "source": [
    "Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40168315",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = train_labels[train_labels['label']==0].sample(TRAIN_SIZE,random_state=45)\n",
    "train_pos = train_labels[train_labels['label']==1].sample(TRAIN_SIZE,random_state=45)\n",
    "\n",
    "train_data = pd.concat([train_neg, train_pos], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_data = shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88d35fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8000\n",
       "1    8000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c97a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ext(fn):\n",
    "    return fn+\".tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20859e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 2)\n",
      "(3200, 2)\n"
     ]
    }
   ],
   "source": [
    "y = train_data['label']\n",
    "train_df, valid_df = train_test_split(train_data, test_size=0.2, random_state=45, stratify=y)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9adaf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>98fce5ccb515f247222119444bef9685ab922f24.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>add659452bd3f2e9884d3cf403554373c730ebd0.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>82fab414ce99ef2f2788bce678ecb97ebbbe1638.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15591</th>\n",
       "      <td>904cfe084edbac4b084472d24c17de5a209e73c6.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>2440ed9a1fa89d8178b66f031cd633e273af9dde.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 id  label\n",
       "9601   98fce5ccb515f247222119444bef9685ab922f24.tif      1\n",
       "195    add659452bd3f2e9884d3cf403554373c730ebd0.tif      0\n",
       "2830   82fab414ce99ef2f2788bce678ecb97ebbbe1638.tif      0\n",
       "15591  904cfe084edbac4b084472d24c17de5a209e73c6.tif      1\n",
       "6174   2440ed9a1fa89d8178b66f031cd633e273af9dde.tif      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['id'] = train_df['id'].apply(append_ext)\n",
    "valid_df['id'] = valid_df['id'].apply(append_ext)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1b76c",
   "metadata": {},
   "source": [
    "Image generator that load images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80fa3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29809c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12800 validated image filenames belonging to 2 classes.\n",
      "Found 3200 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_path = './train'\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "valid_df['label'] = valid_df['label'].astype(str)\n",
    "\n",
    "train_loader = train_datagen.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    directory = train_path,\n",
    "    x_col = 'id',\n",
    "    y_col = 'label',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 42,\n",
    "    shuffle = True,\n",
    "    class_mode = 'categorical',\n",
    "    target_size = (96,96)\n",
    ")\n",
    "\n",
    "valid_loader = valid_datagen.flow_from_dataframe(\n",
    "    dataframe = valid_df,\n",
    "    directory = train_path,\n",
    "    x_col = 'id',\n",
    "    y_col = 'label',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 42,\n",
    "    shuffle = True,\n",
    "    class_mode = 'categorical',\n",
    "    target_size = (96,96)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6036ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "TR_STEPS = len(train_loader)\n",
    "VA_STEPS = len(valid_loader)\n",
    "\n",
    "print(TR_STEPS)\n",
    "print(VA_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6c605ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential([\n",
    "    Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape=(96,96,3)),\n",
    "    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n",
    "    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n",
    "    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n",
    "    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n",
    "    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n",
    "    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "     Dense(128, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c51f8a",
   "metadata": {},
   "source": [
    "Round one training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f105a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.001)\n",
    "cnn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa3c3a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16d15b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16d15b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6504 - accuracy: 0.7077 - auc_2: 0.7271WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x16e30f280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x16e30f280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "200/200 [==============================] - 160s 798ms/step - loss: 0.6500 - accuracy: 0.7079 - auc_2: 0.7274 - val_loss: 2.3034 - val_accuracy: 0.5000 - val_auc_2: 0.5639\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 160s 799ms/step - loss: 0.5341 - accuracy: 0.7907 - auc_2: 0.8296 - val_loss: 2.2150 - val_accuracy: 0.5734 - val_auc_2: 0.5391\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 145s 724ms/step - loss: 0.5053 - accuracy: 0.8067 - auc_2: 0.8469 - val_loss: 0.8213 - val_accuracy: 0.6850 - val_auc_2: 0.7125\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 134s 667ms/step - loss: 0.4784 - accuracy: 0.8123 - auc_2: 0.8607 - val_loss: 0.6184 - val_accuracy: 0.6650 - val_auc_2: 0.7571\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 159s 796ms/step - loss: 0.4126 - accuracy: 0.8253 - auc_2: 0.8946 - val_loss: 1.7861 - val_accuracy: 0.5097 - val_auc_2: 0.5109\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.3928 - accuracy: 0.8270 - auc_2: 0.9038 - val_loss: 0.6389 - val_accuracy: 0.7194 - val_auc_2: 0.7910\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 141s 704ms/step - loss: 0.3709 - accuracy: 0.8374 - auc_2: 0.9150 - val_loss: 1.6100 - val_accuracy: 0.4009 - val_auc_2: 0.3960\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 117s 580ms/step - loss: 0.3666 - accuracy: 0.8404 - auc_2: 0.9169 - val_loss: 1.0190 - val_accuracy: 0.5222 - val_auc_2: 0.5712\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 127s 632ms/step - loss: 0.3352 - accuracy: 0.8539 - auc_2: 0.9309 - val_loss: 1.2154 - val_accuracy: 0.6162 - val_auc_2: 0.6269\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 117s 582ms/step - loss: 0.3320 - accuracy: 0.8545 - auc_2: 0.9324 - val_loss: 0.6521 - val_accuracy: 0.7956 - val_auc_2: 0.8619\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 131s 656ms/step - loss: 0.2977 - accuracy: 0.8706 - auc_2: 0.9459 - val_loss: 0.3204 - val_accuracy: 0.8666 - val_auc_2: 0.9374\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 141s 702ms/step - loss: 0.2872 - accuracy: 0.8784 - auc_2: 0.9497 - val_loss: 0.3350 - val_accuracy: 0.8541 - val_auc_2: 0.9320\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 140s 702ms/step - loss: 0.2804 - accuracy: 0.8817 - auc_2: 0.9523 - val_loss: 0.3277 - val_accuracy: 0.8606 - val_auc_2: 0.9348\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 126s 629ms/step - loss: 0.2711 - accuracy: 0.8852 - auc_2: 0.9552 - val_loss: 0.3636 - val_accuracy: 0.8481 - val_auc_2: 0.9266\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 117s 583ms/step - loss: 0.2655 - accuracy: 0.8891 - auc_2: 0.9572 - val_loss: 0.3561 - val_accuracy: 0.8550 - val_auc_2: 0.9298\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 114s 569ms/step - loss: 0.2570 - accuracy: 0.8952 - auc_2: 0.9598 - val_loss: 0.3086 - val_accuracy: 0.8769 - val_auc_2: 0.9436\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 112s 559ms/step - loss: 0.2499 - accuracy: 0.8948 - auc_2: 0.9622 - val_loss: 0.3748 - val_accuracy: 0.8462 - val_auc_2: 0.9257\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 117s 583ms/step - loss: 0.2434 - accuracy: 0.9012 - auc_2: 0.9641 - val_loss: 0.3288 - val_accuracy: 0.8647 - val_auc_2: 0.9378\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 117s 582ms/step - loss: 0.2342 - accuracy: 0.9070 - auc_2: 0.9668 - val_loss: 0.3205 - val_accuracy: 0.8672 - val_auc_2: 0.9410\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 121s 602ms/step - loss: 0.2257 - accuracy: 0.9061 - auc_2: 0.9693 - val_loss: 0.3313 - val_accuracy: 0.8625 - val_auc_2: 0.9367\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 124s 619ms/step - loss: 0.2144 - accuracy: 0.9145 - auc_2: 0.9726 - val_loss: 0.3237 - val_accuracy: 0.8712 - val_auc_2: 0.9411\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 123s 612ms/step - loss: 0.2102 - accuracy: 0.9158 - auc_2: 0.9735 - val_loss: 0.3142 - val_accuracy: 0.8750 - val_auc_2: 0.9437\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 157s 784ms/step - loss: 0.2105 - accuracy: 0.9154 - auc_2: 0.9735 - val_loss: 0.3165 - val_accuracy: 0.8756 - val_auc_2: 0.9429\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 213s 1s/step - loss: 0.2072 - accuracy: 0.9205 - auc_2: 0.9742 - val_loss: 0.3177 - val_accuracy: 0.8722 - val_auc_2: 0.9430\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 185s 927ms/step - loss: 0.2083 - accuracy: 0.9167 - auc_2: 0.9739 - val_loss: 0.3193 - val_accuracy: 0.8725 - val_auc_2: 0.9426\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 165s 822ms/step - loss: 0.2084 - accuracy: 0.9171 - auc_2: 0.9739 - val_loss: 0.3186 - val_accuracy: 0.8737 - val_auc_2: 0.9430\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 237s 1s/step - loss: 0.2052 - accuracy: 0.9192 - auc_2: 0.9748 - val_loss: 0.3204 - val_accuracy: 0.8737 - val_auc_2: 0.9431\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 575s 3s/step - loss: 0.2037 - accuracy: 0.9188 - auc_2: 0.9749 - val_loss: 0.3205 - val_accuracy: 0.8734 - val_auc_2: 0.9422\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 390s 2s/step - loss: 0.2019 - accuracy: 0.9194 - auc_2: 0.9757 - val_loss: 0.3221 - val_accuracy: 0.8744 - val_auc_2: 0.9431\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 142s 706ms/step - loss: 0.2024 - accuracy: 0.9202 - auc_2: 0.9754 - val_loss: 0.3198 - val_accuracy: 0.8750 - val_auc_2: 0.9435\n"
     ]
    }
   ],
   "source": [
    "h1 = cnn.fit(\n",
    "    x = train_loader, \n",
    "    steps_per_epoch = TR_STEPS, \n",
    "    epochs = 10,\n",
    "    validation_data = valid_loader, \n",
    "    validation_steps = VA_STEPS, \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "tf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)\n",
    "\n",
    "h2 = cnn.fit(\n",
    "    x = train_loader, \n",
    "    steps_per_epoch = TR_STEPS, \n",
    "    epochs = 10,\n",
    "    validation_data = valid_loader, \n",
    "    validation_steps = VA_STEPS, \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "tf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.00001)\n",
    "\n",
    "\n",
    "h3 = cnn.fit(\n",
    "    x = train_loader, \n",
    "    steps_per_epoch = TR_STEPS, \n",
    "    epochs = 10,\n",
    "    validation_data = valid_loader, \n",
    "    validation_steps = VA_STEPS, \n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c852b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
